{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d392107-c0bf-4e67-b327-2414ae034c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Load your JSON file\n",
    "with open(\"ragas.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "# Convert into ragas format\n",
    "data = {\n",
    "    \"question\": [],\n",
    "    \"contexts\": [],\n",
    "    \"answer\": [],\n",
    "}\n",
    "\n",
    "for item in raw_data:\n",
    "    data[\"question\"].append(item[\"question\"])\n",
    "    data[\"contexts\"].append([c[\"text\"] for c in item[\"context\"]])\n",
    "    data[\"answer\"].append(item[\"answer_gpt\"])\n",
    "\n",
    "# Create a HuggingFace Dataset\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Wrap into DatasetDict as required by ragas\n",
    "dataset = DatasetDict({\"eval\": dataset})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3fad623-734d-4186-8c96-75c8615d6132",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The metric [context_precision] that is used requires the following additional columns ['reference'] to be present in the dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mragas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mragas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m faithfulness, answer_relevancy, context_precision\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m result = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meval\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfaithfulness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswer_relevancy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_precision\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ragas\\_analytics.py:250\u001b[39m, in \u001b[36mtrack_was_completed.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001b[32m    249\u001b[39m     track(IsCompleteEvent(event_type=func.\u001b[34m__name__\u001b[39m, is_completed=\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     track(IsCompleteEvent(event_type=func.\u001b[34m__name__\u001b[39m, is_completed=\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[32m    253\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ragas\\evaluation.py:184\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(dataset, metrics, llm, embeddings, experiment_name, callbacks, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size, _run_id, _pbar, return_executor)\u001b[39m\n\u001b[32m    181\u001b[39m     dataset = EvaluationDataset.from_list(dataset.to_list())\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, EvaluationDataset):\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     \u001b[43mvalidate_required_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m     validate_supported_metrics(dataset, metrics)\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# set the llm and embeddings\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ragas\\validation.py:63\u001b[39m, in \u001b[36mvalidate_required_columns\u001b[39m\u001b[34m(ds, metrics)\u001b[39m\n\u001b[32m     61\u001b[39m available_columns = \u001b[38;5;28mset\u001b[39m(ds.features())\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m required_columns.issubset(available_columns):\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     64\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe metric [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] that is used requires the following \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     65\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33madditional columns \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(required_columns\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mavailable_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     66\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mto be present in the dataset.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     67\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: The metric [context_precision] that is used requires the following additional columns ['reference'] to be present in the dataset."
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy\n",
    "\n",
    "result = evaluate(\n",
    "    dataset[\"eval\"],\n",
    "    metrics=[faithfulness, answer_relevancy],  # no context_precision\n",
    ")\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd5e835-3c96-44ed-89cf-9f6b90e859b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
